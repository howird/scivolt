#### 10.1 Large language models (LLMs) can perform new tasks without fine tuning
- Zero-shot inference: On reading comprehension tasks, GPT-2 was competitive with supervised methods
	- Good performance on Children’s Book Test, a multiple-choice masked language modelling test with children’s books
	- On other tasks such as summarization and question answering, GPT-2 generally wasn’t competitive with supervised models but outperformed simple baselines
- Few-shot inference:
	- GPT-3 (larger version of GPT-2 with 175B params, trained with more data)
	- Evaluated on various language tasks in several settings:
		- Zero-shot: Input sequence includes a prompt to perform the task
		- One-shot: An example of correct task completion included in the input sequence
		- Few-shot: Usually 10 to 100 examples of correct task completion in the input sequence

	- Performance usually improved from zero to one to few-shot performance, with the latter often competitive with strong supervised baselines (rarely state-of-art)
	- Performance improved with model size, and the gap between zero and few-shot performance also increased with model size
- Better performance is still achieved with fine tuning than with few-shot prompting

#### 10.2 LLMs are getting larger
- LLMs have gotten larger by orders of magnitude every year
- Transformer performance improves reliably with the number of parameters and dataset size
	- larger models required fewer training samples to reach the same performance
	- performance was less sensitive to model shape, e.g.:
		- Dimension of FF layers relative to embeddings
		- Ratio of embedding dimension to number of layers
		- Ratio of embedding dimension to number of attention heads
	- large models are undertrained:
		- With increasing compute budget, model size and data size should scale at the same rate
- Emerging capabilities:
	- Various qualitatively new capabilities have emerged with increasing scale
	- It is not yet predictable which capabilities will emerge at which scale

#### 10.3 LLMs can produce convincing output
- People asked to guess whether 500-word news articles were more likely written by humans or generated by a machine
	- Near chance performance on articles written by full-size GPT-3
- Google employee, Blake Lemoine, thought Google's LaMDA was sentient and went public with this belief
- Meta's Galactica was taken offline after three days due to widespread concern that it produced convincing but incorrect text
- Flan-PaLM and GPT-3.5 performed around a passing level on the US Medical Licensing Exam
	- Med-PaLM 2 and GPT-4 performed at an expert level
- GPT-4 outperforms law students on the multistate bar exam (multiple choice component of Uniform Bar Exam)
	- passes other components including an essay component
- GPT-4 scores in standard exams without specific training for each one

#### 10.4 Causal language modelling isn’t enough to make LLMs useful
- RLHF (iterated several times with outputs of improving models):
	- Humans prompt the model, generate two responses (from different model variants for diversity), and indicate which they like better and the strength of their preference; different annotators produce ratings based on either response safety or helpfulness
	- Separate models trained based on these responses to produce safety and helpfulness scores based on prompt and response
	- Reinforcement learning methods (including proximal policy optimization) are used to update the model based on these scores

#### 10.5 LLMs can be fine-tuned using parameter-efficient updates